{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns and their data types\n",
    "dtype = {\n",
    "    'code': 'str',\n",
    "    'product_name': 'str',\n",
    "    'brands': 'str',\n",
    "    'categories_en': 'str',\n",
    "    'labels_en': 'str',\n",
    "    'serving_size': 'str',\n",
    "    'fat_100g': 'str',  # Initially set to str\n",
    "    'carbohydrates_100g': 'str',  # Initially set to str\n",
    "    'proteins_100g': 'str',  # Initially set to str\n",
    "    'ingredients_text': 'str',\n",
    "    'additives_en': 'str',\n",
    "    'labels': 'str',\n",
    "    'traces_en': 'str',\n",
    "    'nutrition-score-fr_100g': 'str',  # Initially set to str\n",
    "    'nutriscore_score': 'str',  # Initially set to str\n",
    "    'completeness': 'str'  # Initially set to str\n",
    "}\n",
    "\n",
    "# Columns to be read\n",
    "usecols = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories_en',\n",
    "    'labels_en',\n",
    "    'serving_size',\n",
    "    'fat_100g',\n",
    "    'carbohydrates_100g',\n",
    "    'proteins_100g',\n",
    "    'ingredients_text',\n",
    "    'additives_en',\n",
    "    'labels',\n",
    "    'traces_en',\n",
    "    'nutrition-score-fr_100g',\n",
    "    'nutriscore_score',\n",
    "    'completeness',\n",
    "    'countries_en'  # Needed for filtering later\n",
    "]\n",
    "\n",
    "# Read the first 100 rows with Pandas\n",
    "pandas_df = pd.read_csv('dataset.csv', delimiter='\\t', dtype=dtype, usecols=usecols, on_bad_lines='skip')\n",
    "\n",
    "# Filter the DataFrame for entries in the United States\n",
    "us_food_df = pandas_df[pandas_df['countries_en'].str.contains('United States', na=False)]\n",
    "\n",
    "# Select the relevant columns\n",
    "selected_columns = [\n",
    "    'code',\n",
    "    'product_name',\n",
    "    'brands',\n",
    "    'categories_en',\n",
    "    'labels_en',\n",
    "    'serving_size',\n",
    "    'fat_100g',\n",
    "    'carbohydrates_100g',\n",
    "    'proteins_100g',\n",
    "    'ingredients_text',\n",
    "    'additives_en',\n",
    "    'labels',\n",
    "    'traces_en',\n",
    "    'nutrition-score-fr_100g',\n",
    "    'nutriscore_score',\n",
    "    'completeness'\n",
    "]\n",
    "\n",
    "us_food_df = us_food_df[selected_columns]\n",
    "\n",
    "# Output the filtered DataFrame to a CSV file\n",
    "us_food_df.to_csv('us_food_data.csv', index=False)\n",
    "    \n",
    "    \n",
    "for col in ['fat_100g', 'carbohydrates_100g', 'proteins_100g', 'nutrition-score-fr_100g', 'nutriscore_score', 'completeness']:\n",
    "    us_food_df[col] = pd.to_numeric(us_food_df[col], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the filtered CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_df = pd.read_csv('us_food_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cleaning up dataset \n",
    "\n",
    "1. selecting columns that are relevant to macro calculator \n",
    "2. removing any serving size from dataset \n",
    "3. removing any macros where all three are less than 0\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "food_df_cleaned = food_df.copy()\n",
    "food_df_cleaned = food_df_cleaned[food_df['serving_size'].notna()]\n",
    "\n",
    "food_df_cleaned = food_df_cleaned[\n",
    "    (food_df_cleaned['carbohydrates_100g'] + food_df_cleaned['proteins_100g'] + food_df_cleaned['fat_100g']) > 0\n",
    "].reset_index(drop=True)\n",
    "\n",
    "food_df_cleaned = food_df_cleaned[\n",
    "    (food_df_cleaned['nutrition-score-fr_100g'] >= -15) & (food_df_cleaned['nutrition-score-fr_100g'] <= 18)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_diet(fats, carbohydrates, protein): \n",
    "    \"\"\"\n",
    "    Categorizes a food item into different diet types based on its macronutrient composition.\n",
    "\n",
    "    This function calculates the percentage of fats, carbohydrates, and proteins in the total macronutrient content of a food item. \n",
    "    Based on these percentages, it categorizes the food item into one or more diet types: 'Balanced', 'Keto', 'High Protein', and 'Low Fat'.\n",
    "\n",
    "    Args:\n",
    "        fats (float): The amount of fats in 100 grams of the food item.\n",
    "        carbohydrates (float): The amount of carbohydrates in 100 grams of the food item.\n",
    "        protein (float): The amount of proteins in 100 grams of the food item.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of diet types that the food item fits into. If no diet type is matched, it returns ['None'].\n",
    "\n",
    "    Diet Type Criteria:\n",
    "        - 'Balanced': 30% to 70% carbohydrates, 10% to 35% proteins, and 20% to 40% fats.\n",
    "        - 'Keto': At least 60% fats and more protein than carbohydrates.\n",
    "        - 'High Protein': At least 30% proteins.\n",
    "        - 'Low Fat': Less than 20% fats.\n",
    "    \"\"\"\n",
    "    total_macros = fats + carbohydrates + protein\n",
    "\n",
    "    fats_ratio = (fats / total_macros) * 100 if total_macros > 0 else 0\n",
    "    carbs_ratio = (carbohydrates / total_macros) * 100 if total_macros > 0 else 0\n",
    "    protein_ratio = (protein / total_macros) * 100 if total_macros > 0 else 0\n",
    "\n",
    "    diets = []\n",
    "\n",
    "    if total_macros > 5 and carbs_ratio <= 70:  # Adjust this threshold as needed\n",
    "        if 30 <= carbs_ratio <= 70 and 10 <= protein_ratio <= 35 and 20 <= fats_ratio <= 40:\n",
    "            diets.append('Balanced')\n",
    "        \n",
    "        if fats_ratio >= 60 and protein >= carbohydrates:\n",
    "            diets.append('Keto')\n",
    "        \n",
    "        if protein_ratio >= 30:\n",
    "            diets.append('High Protein')\n",
    "        \n",
    "        if fats_ratio < 20:\n",
    "            diets.append('Low Fat')\n",
    "\n",
    "\n",
    "    return diets if diets else ['None']\n",
    "\n",
    "food_df_cleaned['diet_types'] = food_df_cleaned.apply(lambda row: categorize_diet(row['fat_100g'], row['carbohydrates_100g'], row['proteins_100g']), axis=1)\n",
    "\n",
    "food_df_cleaned_diet = food_df_cleaned[food_df_cleaned['diet_types'].apply(lambda x: 'None' not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" \n",
    "    Removes any accent marks within text to get only English words.\n",
    "\n",
    "    Args:\n",
    "        text (string): The string containing the words to be checked and cleaned.\n",
    "\n",
    "    Returns:\n",
    "        string: The cleaned text with only English characters and spaces.\n",
    "    \"\"\"\n",
    "    text = unidecode.unidecode(str(text))\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "food_df_cleaned_diet_copy = food_df_cleaned_diet.copy()\n",
    "\n",
    "for column in ['product_name', 'brands', 'categories_en', 'ingredients_text']:\n",
    "    food_df_cleaned_diet_copy.loc[:, column] = food_df_cleaned_diet_copy[column].apply(clean_text)\n",
    "\n",
    "food_df_cleaned_diet = food_df_cleaned_diet_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allergen_dict = {\n",
    "    'nuts': ['almond', 'walnut', 'hazelnut', 'cashew', 'pistachio', 'brazil nut', 'pecan', 'macadamia', 'nut'],\n",
    "    'peanuts': ['peanut'],\n",
    "    'dairy': ['milk', 'cheese', 'butter', 'yogurt', 'cream', 'lactose', 'whey', 'casein'],\n",
    "    'eggs': ['egg'],\n",
    "    'soy': ['soy', 'soybean', 'tofu', 'tempeh', 'edamame'],\n",
    "    'wheat': ['wheat', 'gluten', 'barley', 'rye', 'bread', 'pasta', 'flour'],\n",
    "    'fish': ['fish', 'salmon', 'tuna', 'cod', 'haddock'],\n",
    "    'shellfish': ['shrimp', 'crab', 'lobster', 'clam', 'mussel', 'oyster', 'scallop'],\n",
    "    'sesame': ['sesame', 'tahini'],\n",
    "}\n",
    "\n",
    "diet_keywords = {\n",
    "    'vegan': ['plant-based', 'vegan', 'dairy-free', 'egg-free'],\n",
    "    'vegetarian': ['vegetarian', 'egg', 'milk', 'cheese', 'yogurt', 'plant-based'],\n",
    "}\n",
    "\n",
    "\n",
    "columns = ['product_name', 'categories_en', 'labels_en', 'ingredients_text', 'diet_types']\n",
    "\n",
    "diet_perf_df = food_df_cleaned_diet[columns]\n",
    "\n",
    "combined_allergens = [\n",
    "    'almond', 'walnut', 'hazelnut', 'cashew', 'pistachio', 'brazil nut', 'pecan', 'macadamia', 'nut',\n",
    "    'peanut', 'milk', 'cheese', 'butter', 'yogurt', 'cream', 'lactose', 'whey', 'casein', 'egg',\n",
    "    'soy', 'soybean', 'tofu', 'tempeh', 'edamame', 'wheat', 'gluten', 'barley', 'rye', 'bread', 'pasta', 'flour',\n",
    "    'fish', 'salmon', 'tuna', 'cod', 'haddock', 'shrimp', 'crab', 'lobster', 'clam', 'mussel', 'oyster', 'scallop',\n",
    "    'sesame', 'tahini'\n",
    "]\n",
    "\n",
    "def may_contain_allergens(product_name, categories, labels, ingredients):\n",
    "    \"\"\"\n",
    "    Combines relevant text fields and checks for the presence of common allergens.\n",
    "\n",
    "    Args:\n",
    "        product_name (string): The name of the product.\n",
    "        categories (string): The categories the product belongs to.\n",
    "        labels (string): The labels associated with the product.\n",
    "        ingredients (string): The list of ingredients in the product.\n",
    "\n",
    "    Returns:\n",
    "        string: A warning message if common allergens are detected, otherwise a message indicating no common allergens.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_text = ' '.join(filter(None, [str(product_name).lower(), str(categories).lower(), str(labels).lower(), str(ingredients).lower()]))\n",
    "\n",
    "    if any(allergen in combined_text for allergen in combined_allergens):\n",
    "        return 'May contain common allergens'\n",
    "    else:\n",
    "        return 'No common allergens detected'\n",
    "\n",
    "food_df_cleaned_allergens = food_df_cleaned_diet.copy()\n",
    "food_df_cleaned_allergens['allergen_warning'] = food_df_cleaned_allergens.apply(\n",
    "    lambda row: may_contain_allergens(\n",
    "        row['product_name'], row['categories_en'], row['labels_en'], row['ingredients_text']\n",
    "    ), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_meal(row):\n",
    "    \"\"\"\n",
    "    Categorizes a food item based on keywords found in its product name, categories, and ingredients.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series): A row of the DataFrame containing the product details.\n",
    "\n",
    "    Returns:\n",
    "        string: The meal category, which can be 'breakfast', 'snack', 'not_meal', or 'lunch_dinner'.\n",
    "    \"\"\"\n",
    "\n",
    "    meal_keywords = {\n",
    "        'breakfast': ['cereal', 'oatmeal', 'yogurt', 'eggs', 'granola', 'breakfast'],\n",
    "        'snack': ['snack', 'nuts', 'cookie', 'chips', 'bar', 'crisps', 'biscuits', 'cookies', 'supplements', 'powder', 'beverage', 'drink', 'milk', 'soup', 'dessert'],\n",
    "        'not_meal': ['canned', 'flour', 'seeds', 'cocoa', 'condiment', 'oil', 'sauce', 'dressing', 'condiment', 'spread', 'sauces', 'condiments']\n",
    "    }\n",
    "    \n",
    "    combined_text = \" \".join([str(row.get(column, '')).lower() for column in ['product_name', 'categories_en', 'ingredients_text']])\n",
    "\n",
    "    if any(keyword in combined_text for keyword in meal_keywords['not_meal']):\n",
    "        return 'not_meal'\n",
    "    \n",
    "    if any(keyword in combined_text for keyword in meal_keywords['snack']):\n",
    "        return 'snack'\n",
    "    \n",
    "    if 'pasta' in combined_text or 'lentil' in combined_text:\n",
    "        return 'lunch_dinner'\n",
    "    \n",
    "    if any(keyword in combined_text for keyword in meal_keywords['breakfast']):\n",
    "        return 'breakfast'\n",
    "\n",
    "    return 'lunch_dinner'\n",
    "\n",
    "food_df_cleaned_meals = food_df_cleaned_diet.copy()\n",
    "food_df_cleaned_meals['meal_category'] = food_df_cleaned_meals.apply(categorize_meal, axis=1)\n",
    "\n",
    "food_df_cleaned_meals[['product_name', 'categories_en', 'ingredients_text', 'meal_category']].head(100)\n",
    "\n",
    "food_types = ['lunch_dinner', 'breakfast', 'snack'] \n",
    "\n",
    "food_df_cleaned_meals = food_df_cleaned_meals[food_df_cleaned_meals['meal_category'].isin(food_types)]\n",
    "food_df_cleaned_meals.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspi_avoid_additives = [\n",
    "    \"Acesulfame potassium\",\n",
    "    \"Aloe vera\",\n",
    "    \"Aspartame\",\n",
    "    \"Azodicarbonamide\",\n",
    "    \"Brominated vegetable oil\",\n",
    "    \"bva\",\n",
    "    \"bho\",\n",
    "    \"Butylated hydroxyanisole\",\n",
    "    \"Caramel coloring\",\n",
    "    \"Cyclamate\",\n",
    "    \"Ginkgo biloba\",\n",
    "    \"Olestra (olean)\",\n",
    "    \"Potassium bromate\",\n",
    "    \"Potassium iodate\",\n",
    "    \"Propyl gallate\",\n",
    "    \"Saccharin\",\n",
    "    \"Sodium nitrate\",\n",
    "    \"Sucralose\",\n",
    "    \"TBHQ\",\n",
    "    \"tert-butylhydroquinone\",\n",
    "    \"Titanium dioxide\",\n",
    "    \"Trans fat\",\n",
    "    \"Red 40\", \"E129\",\n",
    "    \"Yellow 5\", \"E102\",\n",
    "    \"Yellow 6\", \"E110\",\n",
    "    \"Blue 1\", \"E133\",\n",
    "    \"Blue 2\", \"E132\",\n",
    "    \"Green 3\", \"E143\",\n",
    "    \"Orange B\", \"E110\",\n",
    "    \"Red 3\", \"E127\"\n",
    "]\n",
    "\n",
    "cspi_avoid_additives = [additive.lower() for additive in cspi_avoid_additives]\n",
    "\n",
    "def check_additives(additives_str):\n",
    "    \"\"\"\n",
    "    Checks if any additives in the provided string are on the CSPI avoid list.\n",
    "\n",
    "    Args:\n",
    "        additives_str (string): A comma-separated string of additives.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if any additive is on the CSPI avoid list, otherwise False.\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(additives_str):\n",
    "        return False\n",
    "    additives_list = [additive.strip().lower() for additive in additives_str.split(',')]\n",
    "    for additive in additives_list:\n",
    "        if any(avoid_additive in additive for avoid_additive in cspi_avoid_additives):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "food_df_cleaned_additives = food_df_cleaned_meals.copy() \n",
    "\n",
    "food_df_cleaned_additives = food_df_cleaned_additives[food_df_cleaned_additives['additives_en'].apply(check_additives) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_categories(cat_str):\n",
    "    \"\"\"\n",
    "    Cleans and processes a comma-separated string of categories.\n",
    "\n",
    "    Args:\n",
    "        cat_str (string): A comma-separated string containing category names.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned category names with stopwords removed and words converted to lowercase.\n",
    "    \"\"\"\n",
    "    if not isinstance(cat_str, str):\n",
    "        return []\n",
    "    categories = cat_str.lower().split(',')\n",
    "    cleaned_categories = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for category in categories:\n",
    "        words = [word for word in category.strip().split() if word not in stop_words]\n",
    "        unique_words = []\n",
    "        [unique_words.append(word) for word in words if word not in unique_words]\n",
    "        cleaned_category = ' '.join(unique_words)\n",
    "        cleaned_categories.append(cleaned_category)\n",
    "\n",
    "    return cleaned_categories\n",
    "\n",
    "food_df_cleaned_cat = food_df_cleaned_additives.copy()\n",
    "food_df_cleaned_cat['cleaned_categories'] = food_df_cleaned_cat['categories_en'].apply(clean_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_categories(categories):\n",
    "    if isinstance(categories, list):\n",
    "        categories = ' '.join(categories)\n",
    "    \n",
    "    return [ps.stem(word) for word in categories.split()]\n",
    "\n",
    "#filter to non null values \n",
    "food_df_clean_cat_porter = food_df_cleaned_cat.dropna(subset=['categories_en']).copy()\n",
    "food_df_clean_cat_porter['category_cleaned_stemmed'] = food_df_clean_cat_porter['cleaned_categories'].apply(stem_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_categories(categories):\n",
    "    if isinstance(categories, list):\n",
    "        categories = ' '.join(categories)\n",
    "    return [lemmatizer.lemmatize(word) for word in categories.split()]\n",
    "food_df_clean_cat_lem = food_df_clean_cat_porter.copy()\n",
    "food_df_clean_cat_lem['category_cleaned_lemmatizer'] = food_df_clean_cat_lem['cleaned_categories'].apply(stem_categories)\n",
    "\n",
    "food_df_clean_cat_lem.drop(columns=['category_cleaned_lemmatizer', 'category_cleaned_stemmed'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert list of words back into a string\n",
    "def join_words(word_list):\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "food_df = food_df_clean_cat_lem.copy()\n",
    "food_df['categories_str'] = food_df['cleaned_categories'].apply(join_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_accent_marks(text):\n",
    "    \"\"\"\n",
    "    Check if the text contains any accent marks.\n",
    "    \"\"\"\n",
    "    accent_pattern = re.compile(r'[\\u00C0-\\u024F]')\n",
    "    return bool(accent_pattern.search(str(text)))\n",
    "\n",
    "food_df = food_df[~food_df['product_name'].apply(contains_accent_marks) & ~food_df['ingredients_text'].apply(contains_accent_marks)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Machine Learning to Cluster Foods to Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "\n",
    "ngram_ranges = [(1, 1), (1, 2), (2, 2)]\n",
    "max_dfs = [0.5, 0.75, 1.0]\n",
    "min_dfs = [1, 2, 5]\n",
    "\n",
    "best_score = -1\n",
    "best_config = {}\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    for max_df in max_dfs:\n",
    "        for min_df in min_dfs:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=max_df, min_df=min_df)\n",
    "            X = vectorizer.fit_transform(food_df['categories_str'])\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=12, n_init=10, random_state=42)\n",
    "            cluster_labels = kmeans.fit_predict(X)\n",
    "            \n",
    "            score = silhouette_score(X, cluster_labels)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_config = {\n",
    "                    'ngram_range': ngram_range,\n",
    "                    'max_df': max_df,\n",
    "                    'min_df': min_df\n",
    "                }\n",
    "\n",
    "print(f\"Best Score: {best_score}\")\n",
    "print(f\"Best Configuration: {best_config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = best_config['ngram_range']\n",
    "max_df = best_config['max_df']\n",
    "min_df = best_config['min_df']\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=ngram_range, max_df=max_df, min_df=min_df)\n",
    "X_tfidf = tfidf.fit_transform(food_df['categories_str'])\n",
    "\n",
    "svd = TruncatedSVD(n_components=12, random_state=42)\n",
    "X = svd.fit_transform(X_tfidf)\n",
    "\n",
    "best_score = -1\n",
    "best_k = 0\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(5, 20):  \n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    if silhouette_avg > best_score:\n",
    "        best_score = silhouette_avg\n",
    "        best_k = k\n",
    "\n",
    "print(f\"The best silhouette score is {best_score} for k = {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "best_n_componenets = 12\n",
    "svd = TruncatedSVD(n_components=best_n_componenets, random_state=42)\n",
    "\n",
    "\n",
    "X_reduced = svd.fit_transform(X)\n",
    "\n",
    "db_index = davies_bouldin_score(X_reduced, cluster_labels)\n",
    "ch_score = calinski_harabasz_score(X_reduced, cluster_labels)\n",
    "silhouette_avg = silhouette_score(X_reduced, cluster_labels)\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {db_index}\")\n",
    "print(f\"Calinski-Harabasz Score: {ch_score}\")\n",
    "print(f\"The average silhouette_score is : {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "best_ngram_range = best_config['ngram_range']  \n",
    "best_max_df = best_config['max_df']      \n",
    "best_min_df = best_config['min_df']     \n",
    "best_n_clusters = best_k    \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=best_ngram_range,\n",
    "                              max_df=best_max_df,\n",
    "                              min_df=best_min_df)),\n",
    "    ('svd', TruncatedSVD(n_components=best_n_componenets, random_state=42)),  \n",
    "])\n",
    "\n",
    "pipeline.fit(food_df['categories_str'])\n",
    "\n",
    "cluster_labels = pipeline.predict(food_df['categories_str'])\n",
    "\n",
    "food_df['cluster'] = cluster_labels\n",
    "\n",
    "# Validate clusters\n",
    "for cluster in range(best_n_clusters):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    print(food_df[food_df['cluster'] == cluster]['product_name'].head(10)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_examples_from_clusters(df, num_examples=10):\n",
    "    for cluster_id in range(best_n_clusters):\n",
    "        print(f\"Examples from Cluster {cluster_id}:\")\n",
    "        cluster_data = df[df['cluster'] == cluster_id]['categories_str'].head(num_examples)\n",
    "        for example in cluster_data:\n",
    "            print(f\" - {example}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "columns = ['cluster', 'diet_types', 'cleaned_categories']\n",
    "cluster_label_df = food_df[columns].copy()\n",
    "\n",
    "clusters = cluster_label_df['cluster'].unique()\n",
    "\n",
    "clusters_top_kw = {}\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_categories = cluster_label_df[cluster_label_df['cluster'] == cluster]['cleaned_categories']\n",
    "    \n",
    "    all_words = [] \n",
    "    for categories in cluster_categories:\n",
    "\n",
    "        all_words.extend(categories)  \n",
    "\n",
    "    word_count = Counter(all_words)\n",
    "\n",
    "    top_words = word_count.most_common(10)\n",
    "\n",
    "    clusters_top_kw[cluster] = top_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through list of top keywords to name each cluster group, results will change if you change components and cluster groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_titles = [\n",
    "    \"Cheeses and Alternatives\",  # 0\n",
    "    \"Mixed Plant-Based Foods\",  # 1\n",
    "    \"Prepared Meats\",  # 2\n",
    "    \"Fermented Dairy Desserts\",  # 3\n",
    "    \"Plant-Based Fruits and Vegetables\",  # 4\n",
    "    \"Frozen Foods\",  # 5\n",
    "    \"Greek-Style Yogurts\",  # 6\n",
    "    \"Meats\",  # 7\n",
    "    \"Cheddar Cheeses\",  # 8\n",
    "    \"Milks, Snacks, and Soups\",  # 9\n",
    "    \"Sausages\",  # 10\n",
    "    \"Frozen Poultry\",  # 11\n",
    "    \"Eggs\",  # 12\n",
    "    \"Whole Yogurts\",  # 13\n",
    "    \"Skimmed Milks\",  # 14\n",
    "    \"Nuts and Cereals\",  # 15\n",
    "    \"Smoked Sausages\",  # 16\n",
    "    \"Frozen Chicken Products\",  # 17\n",
    "    \"Frozen Vegetables\",  # 18\n",
    "]\n",
    "\n",
    "cluster_to_diet_map = {\n",
    "    \"Cheeses and Alternatives\": [\"Vegetarian\"],  # 0\n",
    "    \"Mixed Plant-Based Foods\": [\"Vegan\", \"Vegetarian\"],  # 1\n",
    "    \"Prepared Meats\": [\"Paleo\"],  # 2\n",
    "    \"Fermented Dairy Desserts\": [\"Vegetarian\"],  # 3\n",
    "    \"Plant-Based Fruits and Vegetables\": [\"Vegan\", \"Vegetarian\"],  # 4\n",
    "    \"Frozen Foods\": [\"Vegan\", \"Vegetarian\"],  # 5\n",
    "    \"Greek-Style Yogurts\": [\"Vegetarian\"],  # 6\n",
    "    \"Meats\": [\"Paleo\"],  # 7\n",
    "    \"Cheddar Cheeses\": [\"Vegetarian\"],  # 8\n",
    "    \"Milks, Snacks, and Soups\": [\"Vegan\", \"Vegetarian\"],  # 9\n",
    "    \"Sausages\": [\"Paleo\"],  # 10\n",
    "    \"Frozen Poultry\": [\"Paleo\"],  # 11\n",
    "    \"Eggs\": [\"Paleo\"],  # 12\n",
    "    \"Whole Yogurts\": [\"Vegetarian\"],  # 13\n",
    "    \"Skimmed Milks\": [\"Vegetarian\"],  # 14\n",
    "    \"Nuts and Cereals\": [\"Vegan\", \"Vegetarian\"],  # 15\n",
    "    \"Smoked Sausages\": [\"Paleo\"],  # 16\n",
    "    \"Frozen Chicken Products\": [\"Paleo\"],  # 17\n",
    "    \"Frozen Vegetables\": [\"Vegan\", \"Vegetarian\"],  # 18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_df = food_df.copy()\n",
    "cluster_title_dict = {i: title for i, title in enumerate(cluster_titles)}\n",
    "\n",
    "food_df['cluster_labels'] = food_df['cluster'].replace(cluster_title_dict)\n",
    "\n",
    "food_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "food_df['diet_restrictions'] = food_df['cluster_labels'].map(lambda x: cluster_to_diet_map.get(x, []))\n",
    "\n",
    "def combine_diet_types(row):\n",
    "    existing_diets = row['diet_types']\n",
    "    diet_restrictions = row['diet_restrictions']\n",
    "    combined_diets = list(set(existing_diets + diet_restrictions))\n",
    "    combined_diets.append('Balanced')  \n",
    "    return list(set(combined_diets))  \n",
    "food_df['final_diet_types'] = food_df.apply(combine_diet_types, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output results into csv to use again when calculating meal plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_df = pd.to_csv('us_food_data_filtered.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
